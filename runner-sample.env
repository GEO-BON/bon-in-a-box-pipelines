# The following properties will be accessed by the scripts and server at runtime.
# Those marked as conditional will be required only by some scripts, so they might not be necessary depending on the pipelines that you intend to run.
# BON in a Box server needs to be restarted for any change of environment variable to take effect.


# Access GBIF API (conditional)
GBIF_USER=
GBIF_PWD=
GBIF_EMAIL=

# Access the planetary computer APIs (conditional)
JUPYTERHUB_API_TOKEN=
DASK_GATEWAY__AUTH__TYPE=
DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE=
DASK_GATEWAY__ADDRESS=
DASK_GATEWAY__PROXY_ADDRESS=

# Access Red List Index (conditional)
IUCN_TOKEN=

# Copernicus Data Space Ecosystem (conditional)
CDSE_CLIENT_ID=
CDSE_CLIENT_SECRET=

# Allows you to save the pipeline directly to the server in the Pipeline Editor.
# This API can be blocked on some infrastructures by changing this value.
# - deny: Saving will be rejected by server. In the UI, "Save to clipboard" button can be used.
# - allow, or anything else: "Save" and "Save as" buttons available in the UI.
SAVE_PIPELINE_TO_SERVER=allow

# Script server cache option:
# - full: when a script file changes, all related caches are cleared.
# - partial: when a script file changes, cache is only overriden when running with the same inputs.
# NB: applied on next docker compose up
SCRIPT_SERVER_CACHE_CLEANER=partial

# Optional: By default, server starts with address 127.0.0.1.
# it's a loopback address. This ensures that the local machine can access
# the servers, but no other machines on the same network can.
# To publish ports externally, like for server setup, uncomment this to set to 0.0.0.0
#HTTP_ADDRESS=0.0.0.0

# Optional: By default, server starts on localhost with port 80.
# If port 80 is already in use on your server, specify another port here.
# The UI will be visible in http://localhost:81 if HTTP_PORT=81
#HTTP_PORT=81

# Optional: In order to allow only viewing results, and no running,
# this option can be set to true.
#BLOCK_RUNS=true

################
##### HPC ######
################
# Uncomment the following HPC_ variables to connect to a High Performance Computer (HPC).
# An SSH tunnel will be used to send jobs.
# This has currently only been tested with the DRAC infrastructure.

# Config file for the SSH connection to the HPC.
# This file will be stored in a "docker secret" accessible to the script server.
# It should contain only the SSH configuration to connect to the HPC,
# and should be a separate file from your regular SSH config file, if any.
# Example: /home/user/.ssh/biab-config
HPC_SSH_CONFIG_FILE=

# Name of the SSH config to use, as found in the HPC_SSH_CONFIG_FILE.
# Example: robot.hpc.domain.com
HPC_SSH_CONFIG_NAME=

# The ssh PRIVATE key to use when creating the ssh tunnel.
# This key will be stored in a "docker secret" accessible to the script server.
# Preferably, use a key that is dedicated to the connection between your host and the target HPC.
# A new SSH key pair can be generated with `ssh-keygen -t ed25519`,
# and should then be added to the HPC's authorized_keys.
# Example: ~/.ssh/id_ed25519_hpc
HPC_SSH_KEY=

# Path to the known hosts file that includes the HPC's connection node.
# It will be stored in a "docker secret" accessible to the script server.
# Example: ~/.ssh/known_hosts
HPC_KNOWN_HOSTS_FILE=

# Root folder that will be used for BON in a Box-related files on the HPC.
# Make sure that you user has write access to that folder.
# Example: /home/user/bon-in-a-box
HPC_BIAB_ROOT=

# Account name to use with the sbatch command on the HPC.
# This is required on some HPC infrastructures.
# Example: account-name
HPC_SBATCH_ACCOUNT=

# Load apptainer images on the HPC at server bootup.
# - true: attempt to prepare will start asynchronously when script server starts.
# - false: the preparation will start only when /hpc/prepare API is called (can be launched from the Info tab through the UI).
HPC_AUTO_CONNECT=true

###### END HPC ######