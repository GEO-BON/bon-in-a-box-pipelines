#### Load required packages - libraries to run the script ####

# Install necessary libraries - packages  
packagesPrev<- installed.packages()[,"Package"] # Check and get a list of installed packages in this machine and R version
packagesNeed<- list("magrittr", "terra", "raster", "sf", "fasterize", "pbapply", "this.path", "rjson") # Define the list of required packages to run the script
lapply(packagesNeed, function(x) {   if ( ! x %in% packagesPrev ) { install.packages(x, force=T)}    }) # Check and install required packages that are not previously installed

# Load libraries
packagesList<-list("magrittr", "terra") # Explicitly list the required packages throughout the entire routine. Explicitly listing the required packages throughout the routine ensures that only the necessary packages are listed. Unlike 'packagesNeed', this list includes packages with functions that cannot be directly called using the '::' syntax. By using '::', specific functions or objects from a package can be accessed directly without loading the entire package. Loading an entire package involves loading all the functions and objects 
lapply(packagesList, library, character.only = TRUE)  # Load libraries - packages  


#### Set enviroment variables ####

# Set the 'outputFolder' environment variables
# The environment variable refers to the path of the output folder for the entire routine
# 'outputFolder' allows interaction with the output folder of the routine and access to nearby paths, such as the input folder ('input')

# There are two options to set 'outputFolder'
# Please select only one of the two options, while silencing the other with the '#' syntaxis: 

# Option 1: Setting for production pipeline purposes. This is designed for use in a production environment or workflow.
 Sys.getenv("SCRIPT_LOCATION")

# Option 2: Recommended for debugging purposes to be used as a testing environment. This is designed to facilitate script testing and correction
# outputFolder<- {x<- this.path::this.path();  file_prev<-  paste0(gsub("/scripts.*", "/output", x), gsub("^.*/scripts", "", x)  ); options<- tools::file_path_sans_ext(file_prev) %>% {c(paste0(., ".R"), paste0(., "_R"))}; folder_out<- options %>% {.[file.exists(.)]} %>% {.[which.max(sapply(., function(info) file.info(info)$mtime))]}; folder_final<- list.files(folder_out, full.names = T) %>% {.[which.max(sapply(., function(info) file.info(info)$mtime))]} }




# Set the 'input' environment variables. The 'input' environment contains the specified inputs from the ByB platform.
# The input file 'input.json' is generated by executing the 'Run Script' command in the ByB platform.
input <- rjson::fromJSON(file=file.path(outputFolder, "input.json")) # Load input file

# This section adjusts the input values based on specific conditions to rectify and prevent errors in the input paths
input<- lapply(input, function(x) if(!is.null(x)) if( grepl("/", x) ){
  sub("/output/.*", "/output", outputFolder) %>% dirname() %>%  file.path(x) %>% {gsub("//+", "/", .)}  }else{x} ) # adjust input 1



####  Script body ####
output<- tryCatch({
  
  units::units_options(set_units_mode = "standard")
  
  mtx_distance<-     read.csv(input$mtx_distance, row.names = 1,check.names=FALSE) %>% 
    setNames(gsub("^0+", "", colnames(.))) %>% as.data.frame.matrix()
    
  distance<- input$distance_analysis
  
  spatial_unit_data<- read.csv(input$data_spatial_unit) %>% dplyr::select(c(input$column_spatial_unit, input$column_date, input$column_area)) %>%
    dplyr::filter(!duplicated(input$column_spatial_unit)) %>% dplyr::mutate(spatial_unit= "yes_spatial_unit_data")

  spatial_unit_data$date<- as.Date(spatial_unit_data$created_date, format = input$format_date)
  

  start_date<- {if(input$time_start == "NA"){min(spatial_unit_data$date)}else{input$time_start}} %>% lubridate::floor_date(unit = input$time_interval)
  end_date<- {if(input$time_end == "NA"){max(spatial_unit_data$date)}else{input$time_end}}  %>% lubridate::ceiling_date(unit = input$time_interval)
  
  
  period_time<- paste(timechange:::parse_rounding_unit(input$time_interval), collapse = " "  )
  periods<- seq(start_date, end_date, by = period_time)

  spatial_unit_data$period <- lubridate::floor_date(spatial_unit_data$date, input$time_interval) 
  
  spatial_unit_data_v2<- dplyr::filter(spatial_unit_data, date>=start_date, date<=end_date)
  spatial_unit_data_v2 <- spatial_unit_data_v2[
    order(spatial_unit_data_v2[, "date"], spatial_unit_data_v2[, input$column_spatial_unit]) , ]
  

  ## cunsum
   spatial_unit_area<- lapply(seq(nrow(spatial_unit_data_v2)), function(i){
     new_periods<- periods %>%  {.[which(spatial_unit_data_v2[i,"period"] == .):length(.)]}  
     data_period<- lapply(new_periods, function(j) dplyr::mutate(spatial_unit_data_v2[i,], period= j)) %>% plyr::rbind.fill()
  }) %>% plyr::rbind.fill()

   spatial_unit_no_area<- dplyr::select(spatial_unit_area, c("id_pa", "period", "area_spatial")) %>%
     dplyr::distinct() %>% dplyr::group_by(period) %>%
     dplyr::summarize( area_spatial =  input$area_study_area - sum(area_spatial)) %>% 
     dplyr::mutate(spatial_unit= "no_spatial_unit")
   

   spatial_units_periods<- plyr::rbind.fill(list(spatial_unit_area, spatial_unit_no_area))
   spatial_units_periods <-  spatial_units_periods[ order( spatial_units_periods[, "date"],  spatial_units_periods[, input$column_spatial_unit]), ]
  

  
  #Define the decades reported for the protected areas creation to start the analysis
  decades = unique(spatial_units_periods$period) 
  #Create the final table with the results, the final figure is given per decade
  result = as.data.frame(matrix(NA,ncol = 5, nrow = 0))
  colnames(result) = c("Period" ,"Protcon","Protuncon","Protected","Unprotected")
  

  for (i in as.character(decades)) {
    
    #Filter the decade for the analysis
    decade = spatial_units_periods[spatial_units_periods$period==i,]
    
    #Generate the value for protected and not protected area 
    #Extract the total area of interest
    area_consult = sum(decade$area)
    #Extract the area  of interest that not intersect with any the protected area
    area_no_prot = decade[decade$spatial_unit == "no_spatial_unit",]
    area_no_prot = sum(area_no_prot$area_spatial)
    
    
    #Extract the area for the protected areas that intersect with the area of interest
    area_protect = decade[decade$spatial_unit != "no_spatial_unit",]
    area_protect = sum(area_protect$area_spatial)
    
    #Extract the percentage for unprotected area
    unprotected = ((area_consult - area_protect)/area_consult)*100
    protected = ((area_protect/area_consult))*100
    
    #Generate the value for protected areas connected (protcon)
    #Filter the unique id?s for the protected areas that intersect with the area of interest for the decade i
    table = decade[decade$spatial_unit != "no_spatial_unit",]
    ids_pa = unique(table[,input$column_spatial_unit])
    ids_pas = as.character(ids_pa)
    #Filter the distance matrix with the unique previous id's
    dist_ids_pa = mtx_distance %>% {.[ids_pas,]} %>% dplyr::select(ids_pas) %>% as.matrix()
    
    #Convert the matrix to spatial_units_periods frame
    
    #Remove redundant
    dist_ids_pa_data=  data.frame(rows= rownames( dist_ids_pa)[row( dist_ids_pa)],
                             vars= colnames(dist_ids_pa)[col( dist_ids_pa)],
                             values=  c(dist_ids_pa)  ) %>%  dplyr::filter(! rows == vars) 
    
    #Remove the duplicates distances
    dist_ids_pa_datav2<-   if(nrow(dist_ids_pa_data)>0){
      dist_ids_pa_data[!duplicated(lapply(seq(nrow(dist_ids_pa_data)), function(j) dist_ids_pa_data[j,1:2]  )),]
    } else {dist_ids_pa_data}
    
    
    #Filter only the protected areas that are at a distances less than user distance
    dist_ids_pa_con =dist_ids_pa_datav2[dist_ids_pa_datav2$values<distance, ]
    
    #Extract the unique id?s of protected areas at a distances less than user distance
    pa_union =union(unique(dist_ids_pa_con$row),unique(dist_ids_pa_con$vars))
    
    #Look for the id?s in the spatial_units_periods for filter the information
    areas_pa_conec = decade[decade$id_pa %in% pa_union,]
    areas_conec = sum(areas_pa_conec$area)
    #Extract the percentage of area for the conected protected areas (Protcon)
    protcon = (areas_conec/area_protect)*100
    
    #Generate the area for not connected protected areas 
    #Identify the rows where are the connected protected areas
    rows=which(decade$id_pa %in% pa_union)
    #Filter the table "decade" removing connected protected areas
    
    if(length(rows)==0) {
      areas_pa_no_conec = table
    } else {
      areas_pa_no_conec = table[-rows,]
    }
    
    #Extract the percentage of area for the not connected protected areas (Protuncon)
    areas_no_conec = (sum(areas_pa_no_conec$area))
    protuncon = (areas_no_conec/area_protect)*100
    
    #Create the table with the final results  
    #Create the row with the results for the decade i
    result_p= data.frame(Period = i,Protcon=protcon,Protuncon = protuncon, Protected = protected ,Unprotected=unprotected)
    #Add the previous row to the table final results
    result = rbind.data.frame(result,result_p) 
  }

    
  
  
# Define and export the output values

# Define protcom result output
protcon_result_path<- file.path(outputFolder, "protcon_result.csv") # Define the file path for the 'val_wkt_path' output
write.csv(result, protcon_result_path, row.names = F ) # Write the 'val_wkt_path' output


# Define final output list
output<- list(protcon_result= protcon_result_path)


}, error = function(e) { list(error= conditionMessage(e)) })




#### Outputing result to JSON ####

# Write the output list to the 'output.json' file in JSON format
setwd(outputFolder)
jsonlite::write_json(output, "output.json", auto_unbox = TRUE, pretty = TRUE)