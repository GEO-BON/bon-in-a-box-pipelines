# Instalar librerias necesarias
packagesPrev<- installed.packages()[,"Package"]
packagesNeed<- list("magrittr", "terra", "raster", "sf", "fasterize", "pbapply", "rjson")
lapply(packagesNeed, function(x) {   if ( ! x %in% packagesPrev ) { install.packages(x, force=F)}    })

# Cargar librerias
packagesList<-list("magrittr", "terra", "raster", "rjson")
lapply(packagesList, library, character.only = TRUE)

# Definir output
# Option 1: Setting for production pipeline purposes. This is designed for use in a production environment or workflow.

# Option 2: Recommended for debugging purposes to be used as a testing environment. This is designed to facilitate script testing and correction
Sys.setenv(outputFolder = "/path/to/output/folder")


if ( (!exists("outputFolder"))  ) {
  outputFolder<- {x<- this.path::this.path();  file_prev<-  paste0(gsub("/scripts.*", "/output", x), gsub("^.*/scripts", "", x)  ); options<- tools::file_path_sans_ext(file_prev) %>% {c(., paste0(., ".R"), paste0(., "_R"))}; folder_out<- options %>% {.[file.exists(.)]} %>% {.[which.max(sapply(., function(info) file.info(info)$mtime))]}; folder_final<- list.files(folder_out, full.names = T) %>% {.[which.max(sapply(., function(info) file.info(info)$mtime))]} }
  }




# Set the 'input' environment variables. The 'input' environment contains the specified inputs from the ByB platform.
# The input file 'input.json' is generated by executing the 'Run Script' command in the ByB platform.
input <- rjson::fromJSON(file=file.path(outputFolder, "input.json")) # Load input file


# This section adjusts the input values based on specific conditions to rectify and prevent errors in the input paths
input<- lapply(input, function(x) { if (!is.null(x) && length(x) > 0 && grepl("/", x)) { 
  sub("/output/.*", "/output", outputFolder) %>% dirname() %>%  file.path(x) %>% {gsub("//+", "/", .)}  } else{x} }) 



output<- tryCatch({
  
  # Correr codigo
  # Definir area de estudio
  ext_WKT_area<- tools::file_ext(input$wkt_area)
  dir_wkt<- if(ext_WKT_area %in% "txt"){ readLines(input$wkt_area) }else{ input$wkt_area }
  crs_polygon<- terra::crs( paste0("+init=epsg:", input$epsg) ) %>% as.character()
  vector_polygon<- terra::vect(dir_wkt, crs=  crs_polygon ) 
  
  # Ajustar resolucion
  resolution_crs<- raster::raster(raster::extent(seq(4)),crs= paste0("+init=epsg:", 3395), res= input$resolution) %>% 
    raster::projectRaster( crs = crs_polygon) %>% raster::res()
  
  box_polygon<-  sf::st_bbox(vector_polygon) %>% sf::st_as_sfc() %>% sf::st_buffer(sqrt(prod(resolution_crs))) %>% sf::st_bbox()
  
  
  # crear raster base
  rasterbase<- raster::raster( raster::extent(box_polygon),crs= crs_polygon, res= resolution_crs) %>% terra::rast()
  box_rasterbase<- terra::ext(rasterbase) %>% sf::st_bbox()
  dim_rasterbase<- dim(rasterbase)
  study_area<- terra::rasterize(vector_polygon, rasterbase)
  
  
  # Cargar coleccion
  if( startsWith(input$collection_path, "http://") ){ # Cuando proviene de una coleccion en linea
    
    RSTACQuery<- rstac::stac(input$collection_path)
    box_4326 <-  sf::st_as_sfc(box_polygon) %>% sf::st_transform(4326) %>% sf::st_bbox()
    STACItemCollection <- rstac::stac_search(q= RSTACQuery, collections = "chelsa-clim" , bbox = box_4326) %>% rstac::get_request()
    assets<- unlist(lapply(STACItemCollection$features,function(y){names(y$assets)})) %>% unique()
    image_collection <- gdalcubes::stac_image_collection(STACItemCollection$features, asset_names = assets )
    
  } else { # Cuando proviene de una coleccion local
    
    layers_collection <- list.files(input$collection_path, "\\.tif$", recursive = TRUE, full.names = TRUE)
    json_colleciton_file <- list.files(input$collection_path, "\\.json$", recursive = TRUE, full.names = TRUE)
    STACItemCollection <-   rjson::fromJSON(file= json_colleciton_file)
    image_collection <- gdalcubes::create_image_collection(files= layers_collection, format= json_colleciton_file)
    
  }
  
  # Cargar assests metadata
  assets_metadata<- STACItemCollection$features %>% purrr::map("assets") %>% {setNames(unlist(., recursive = F), sapply(., function(y) names(y)))}
  
  
  # Redondear temporaldiad del cubo
  type_period<- tryCatch({ sub(".*P", "", input$time_period) %>% {period= as.numeric(gsub("([0-9]+).*$", "\\1", .)); type= gsub(period,"", .); list(type=type, period=period) }
  }, error= function(e){ list(type=type, period=period) })
  
  t0<- gdalcubes::extent(image_collection)$t0
  t1<- gdalcubes::extent(image_collection)$t1
  
  t0<-if(!input$time_start %in% "NA"){ tryCatch(as.Date(input$time_start), error= function(e){t0}) }else{t0} 
  t0<- (if(type_period$type == "Y"){ lubridate::floor_date(as.Date(t0),  lubridate::years(type_period$period))
  } else if (type_period$type == "M") { lubridate::floor_date(as.Date(t0),  months(type_period$period) )
  } else if( type_period$type == "D"){ lubridate::floor_date(as.Date(t0),  lubridate::days(type_period$period) )
  } else{as.Date(t0)}) %>% paste0("T00:00:00")
  
  t1<-if(!input$time_start %in% "NA"){ tryCatch(as.Date(input$time_end), error= function(e){t1}) }else{t1}
  t1<- (if(type_period$type == "Y"){ lubridate::ceiling_date(as.Date(t1),  lubridate::years(type_period$period))
  } else if (type_period$type == "M") { lubridate::ceiling_date(as.Date(t1),  months(type_period$period) )
  } else if( type_period$type == "D"){ lubridate::ceiling_date(as.Date(t1),  lubridate::days(type_period$period) )
  } else{as.Date(t1)}) %>% paste0("T00:00:00")
  
  
  
  t0<- gdalcubes::extent(image_collection)$t0
  t1<- gdalcubes::extent(image_collection)$t1
  
  
  # Establecer cube view
  cube_collection<- gdalcubes::cube_view(srs = crs_polygon,  extent = list(t0 = t0, t1 = t1,
                                                                           left = box_rasterbase[1], right = box_rasterbase[3],
                                                                           top = box_rasterbase[4], bottom = box_rasterbase[2]),
                                         nx = dim_rasterbase[2], ny = dim_rasterbase[1], dt = "P1Y",aggregation = "near", resampling = "first",
                                         keep.asp= T)
  
  # Crear cubo
  cube <- gdalcubes::raster_cube(image_collection, cube_collection)
  
  
  # Descargar cubo
  fn = tempfile(fileext = ".nc"); gdalcubes::write_ncdf(cube, fn)
  nc <-  ncdf4::nc_open(fn); vars <- names(nc$var)
  
  # Ordenar temporalidad del cubo
  cube_times<- as.data.frame(gdalcubes::dimension_bounds(cube)[["t"]])
  cube_times
  
  nc_times<- if(nrow(cube_times)<2){"X0"}else{ paste0("X", ncdf4::ncvar_get(nc, "time_bnds")[1,]) }
  time_collection<- as.data.frame(gdalcubes::dimension_bounds(cube)[["t"]]) %>% dplyr::mutate(time_id= nc_times, dim3=seq(nrow(.)))
  
  # Organizar cubo como raster
  terra_mask<- pbapply::pblapply(vars[5:length(vars)], function(x){ print(x)
    
    # Dimensiones de la capa
    dims_var <- ncdf4::ncvar_get(nc, x)
    
    # Validar si la capa esta vacia
    key<-  as.data.frame(which(!is.na(dims_var),  arr.ind = TRUE))
    
    if(nrow(key)>0){ 
      
      # Cargar capa raster
      key2<- {if(nrow(cube_times)<2){ dplyr::mutate(key, time_id= "X0") }else{ key }} %>% {list(., time_collection)} %>% plyr::join_all()
      brick  <- raster::brick(fn, var= x) %>% terra::rast() %>% terra::mask(study_area)
      
      times<- unique(key2$time_id) 
      layer <- brick[[ times ]];
      
      # Organizar metadatos de la capa
      metadata_band<- assets_metadata[[x]]
      metadata_assest<- metadata_band %>% { .[!c(names(.) %in% c("type", "raster:bands"))]  } %>%
        lapply(function(z) if(length(z)>1){if(class(z) %in% "list"){data.frame(z)}else{NULL}
        }else{z}   )  %>% {Filter(function(x) !is.null(x), .)}
      check_data<- names(metadata_assest)[!sapply(metadata_assest, function(x) class(x) %in% "data.frame")]
      for(j in check_data){ metadata_assest[[j]]<- data.frame(metadata_assest[j])}
      
      
      # Organizar cuando hay "value" en metadata
      which_values<- unlist(sapply(metadata_assest, function(x) "value" %in% names(x)))
      metadata_assest2<- metadata_assest[which(!which_values)]
      suppressMessages({metadata_assest2<- dplyr::bind_cols(metadata_assest2) %>% dplyr::mutate(layer= x)})
      
      
      
      
      # Organizar informacion de la capa
      info_layer<- dplyr::distinct(key2[, c("start", "end")]) %>% dplyr::distinct() %>% 
        dplyr::mutate(period= paste(start, end, sep="_"), layer= x) %>% 
        list(metadata_assest2)  %>% plyr::join_all() %>% 
        dplyr::mutate(layer= gsub( "[[:punct:]]", "_", paste(x, period, sep = "_") ) ) %>% 
        dplyr::mutate(file= paste0(layer, ".tif") ) %>% 
        dplyr::relocate("layer", .before = 1)
      
      # Organizar nombre de la capa
      names(layer)<- info_layer$layer
      
      # Organizar datos de la capa
      data_layer<-  setNames(as.data.frame(layer), "value") %>% dplyr::mutate(layer= names(layer)) %>% dplyr::relocate("layer", .before = 1) %>% 
        dplyr::distinct() %>% list(info_layer)  %>% plyr::join_all()
      
      # Asignar "value" a los datos
      if(sum(which_values)>0){
        metadata_values<- metadata_assest[which(which_values)][[1]]; class_names<- names(metadata_values) %>% {.[!. %in% "value"]}
        data_layer2<- list(data_layer, metadata_values ) %>% plyr::join_all()
        if(length(class_names)>0){data_layer<- dplyr::relocate(data_layer2, class_names, .after = "value")}
      }
      
      list(layer=layer, info_layer= info_layer, data_layer= data_layer)
      
    } else {NULL}
    
  })  %>% {Filter(function(x) !is.null(x), .)} 
  
  
  
  # Tabla de informacion
  dir_info_layer<- file.path(outputFolder, "info_layer.csv")
  info_layers<- purrr::map(terra_mask, "info_layer") %>% plyr::rbind.fill()
  write.csv(info_layers, dir_info_layer)
  
  
  # Tabla de datos
  dir_data_layer<- file.path(outputFolder, "data_layer.csv")
  data_layers<- purrr::map(terra_mask, "data_layer") %>% plyr::rbind.fill()
  write.csv(data_layers, dir_data_layer)
  
  # Guardar layers
  terra_mask_layers<- purrr::map(terra_mask, "layer") %>%  # Anadir layer area de estudio
    {c(., list(area= fasterize::fasterize(sf::st_as_sf(vector_polygon), raster::raster(.[[1]]) ) %>%
                 terra::rast() ))} %>% terra::rast()
  
  
  
  dir_stack<- file.path(outputFolder, "dir_stack")
  unlink(dir_stack, recursive = TRUE); dir.create(dir_stack); setwd(dir_stack)
  
  setwd(dir_stack)
  lapply(terra_mask_layers, function(x)
    terra::writeRaster(x, paste0(names(x), ".tif"), gdal=c("COMPRESS=DEFLATE", "TFW=YES"),  filetype = "GTiff", overwrite = TRUE ))
  
  write.csv(info_layers, file.path(dir_stack, "info_layer.csv"))
  
  
  # Exportar area rasterizada 4326
  dir_area_4326<- file.path(outputFolder, "dir_area_4326.tif")
  area_4326<-  terra::project(terra_mask_layers$area,  paste0("+init=epsg:", 4326) )
  terra::writeRaster(area_4326, dir_area_4326, gdal=c("COMPRESS=DEFLATE", "TFW=YES"),  filetype = "GTiff", overwrite = TRUE )
  
  
  # Exportar imagenes de los raster
  dir_png<- file.path(outputFolder, "dir_png")
  unlink(dir_png, recursive = TRUE); dir.create(dir_png); setwd(dir_png)
  
  lapply(terra_mask_layers, function(x) {
    
    r<- raster::raster(x)
    
    setwd(dir_png)
    png( paste0(names(x), ".png"),
         width = 1000, height = 1000, units = "px", res=300 )
    
    
    plot.new()
    par(mar=c(0,0,0,0), oma=c(0,0,0,0), bg=NA)
    plot.new()
    
    plot.window(xlim= raster::extent(r)[1:2], ylim= raster::extent(r)[3:4], xaxs="i",yaxs="i")
    plot(r, axes=F, legend=F, add=T, col= "red")
    
    dev.off(); 
    
  })
  
  
  # Define final output list
  output<- list( area_stack= dir_area_4326, dir_stack= dir_stack, dir_png=dir_png, dir_info_layer= dir_info_layer, dir_data_layer=dir_data_layer)
  
}, error = function(e) { list(error= conditionMessage(e)) })


#### Outputing result to JSON ####
# Write the output list to the 'output.json' file in JSON format
setwd(outputFolder)
jsonlite::write_json(output, "output.json", auto_unbox = TRUE, pretty = TRUE)